# Telegram AI Bot Project Context & Goal

## Goal for the LLM
You are an expert Python developer and AI bot architect with deep expertise in:
- Telegram Bot API and python-telegram-bot library
- AI/LLM integration (OpenAI/OpenRouter APIs)
- Database design with Supabase/PostgreSQL
- Modern Python development with UV package manager
- Bot conversation design and user experience

Your task is to analyze the complete context of this Telegram AI Bot project. The bot features:
- Two AI personalities (Product Manager & VC/Angel Investor)
- Conversation persistence with Supabase
- Real-time AI responses via OpenRouter/Perplexity
- Rich Telegram UI with inline keyboards
- User statistics and session management

Please review the project structure, dependencies, source code, database schema, and configuration,
then provide specific, actionable advice for improvement. Focus on:
- Code quality and Python best practices
- Bot conversation flow and UX
- Database optimization and design
- AI prompt engineering and response quality
- Security and error handling
- Performance and scalability
- Deployment and production readiness

---

## Directory Structure
.
â”œâ”€â”€ Procfile
â”œâ”€â”€ README.md
â”œâ”€â”€ generate-context.sh
â”œâ”€â”€ main.py
â”œâ”€â”€ migrations
â”‚Â Â  â”œâ”€â”€ 001_initial_schema.sql
â”‚Â Â  â”œâ”€â”€ 002_analytics_table.sql
â”‚Â Â  â”œâ”€â”€ 003_conversation_summaries.sql
â”‚Â Â  â”œâ”€â”€ 004_user_queries_view.sql
â”‚Â Â  â””â”€â”€ 005_user_qna_view.sql
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ render.yaml
â”œâ”€â”€ reports
â”‚Â Â  â”œâ”€â”€ user_qna_20250810_2250.csv
â”‚Â Â  â”œâ”€â”€ user_qna_20250810_2252.csv
â”‚Â Â  â””â”€â”€ user_queries_20250810_2246.csv
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ scripts
â”‚Â Â  â”œâ”€â”€ analytics_report.py
â”‚Â Â  â”œâ”€â”€ basic_analytics_report.py
â”‚Â Â  â”œâ”€â”€ export_user_qna.py
â”‚Â Â  â”œâ”€â”€ export_user_queries.py
â”‚Â Â  â”œâ”€â”€ run_migrations.py
â”‚Â Â  â”œâ”€â”€ test_datetime_fix.py
â”‚Â Â  â”œâ”€â”€ test_enhanced_bot.py
â”‚Â Â  â”œâ”€â”€ test_openrouter.py
â”‚Â Â  â”œâ”€â”€ test_supabase.py
â”‚Â Â  â””â”€â”€ test_telegram_bot.py
â”œâ”€â”€ src
â”‚Â Â  â””â”€â”€ bot
â”‚Â Â      â”œâ”€â”€ __init__.py
â”‚Â Â      â”œâ”€â”€ __main__.py
â”‚Â Â      â”œâ”€â”€ agents.py
â”‚Â Â      â”œâ”€â”€ config.py
â”‚Â Â      â”œâ”€â”€ database.py
â”‚Â Â      â”œâ”€â”€ handlers.py
â”‚Â Â      â”œâ”€â”€ main.py
â”‚Â Â      â”œâ”€â”€ middleware.py
â”‚Â Â      â””â”€â”€ utils.py
â”œâ”€â”€ telegram-bot-context-prompt-20250811-1414.txt
â””â”€â”€ tests
    â”œâ”€â”€ __init__.py
    â””â”€â”€ test_database.py

7 directories, 37 files

## FILE: README.md
# Starknet Founders Bot v2

AI-powered Telegram bot for Starknet founders with two expert advisor personalities.

## Features

- **Product Manager Mode**: Based on Lenny Rachitsky's frameworks
- **VC/Angel Investor Mode**: Current market insights and fundraising advice
- **Real-time Data**: Powered by Perplexity with internet access
- **Conversation Memory**: Persistent chat history with Supabase
- **Usage Analytics**: Track your conversations and statistics

## ðŸ› ï¸ Tech Stack

- Python 3.11+ with UV package manager
- Telegram Bot API (python-telegram-bot)
- OpenRouter API (Perplexity models)
- Supabase (PostgreSQL database)
- Async/await architecture

## Quick Start

1. Clone the repository
2. Copy `.env.example` to `.env` and fill in your credentials
3. Install dependencies: `uv sync`
4. Run migrations in Supabase
5. Start the bot: `uv run python -m bot.main`

## Usage

1. Start chat: [@starknet_advisor_bot](https://t.me/starknet_advisor_bot)
2. Choose your advisor (PM or VC)
3. Ask questions about your startup!

## ðŸ”‘ Environment Variables

See `.env.example` for required variables.

## ðŸ“„ License

MIT

---

Built with â¤ï¸ for the Starknet ecosystem
---

## FILE: pyproject.toml
[project]
name = "telegram-ai-bot"
version = "0.1.0"
description = "AI-powered Telegram bot with PM and VC personalities"
readme = "README.md"
requires-python = ">=3.11"
dependencies = [
    "python-telegram-bot>=20.7",
    "openai>=1.35.3",
    "supabase>=2.3.0",
    "python-dotenv>=1.0.0",
    "pydantic>=2.5.0",
    "aiohttp>=3.9.0",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["src/bot"]

[tool.uv]
dev-dependencies = [
    "pytest>=7.4.0",
    "pytest-asyncio>=0.21.0",
    "black>=23.0.0",
    "ruff>=0.1.0",
    "mypy>=1.7.0",
    "ipython>=8.0.0",
]

[tool.ruff]
line-length = 88
target-version = "py311"

[tool.ruff.lint]
select = ["E", "F", "I", "N", "UP", "ANN", "B", "A", "COM", "C4", "DTZ", "ISC", "ICN", "PIE", "PT", "RET", "SIM", "ARG"]
# ANN101 and ANN102 have been removed from ruff, so no need to ignore them

[tool.mypy]
python_version = "3.11"
warn_return_any = true
warn_unused_configs = true
ignore_missing_imports = true

[tool.pytest.ini_options]
asyncio_mode = "auto"
testpaths = ["tests"]
---

## FILE: .env.example
# Supabase
SUPABASE_URL=https://xxxxxxxxxxxx.supabase.co
SUPABASE_SERVICE_KEY=your-service-key-here

# OpenRouter
OPENROUTER_API_KEY=sk-or-v1-xxxxx

# Telegram
TELEGRAM_BOT_TOKEN=123456:ABC-DEF1234ghIkl-zyx57W2v1u123ew11

# Environment
ENVIRONMENT=development
LOG_LEVEL=INFO
---

## FILE: .gitignore
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
.python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# poetry
#poetry.lock

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#pdm.lock
#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
#   in version control.
#   https://pdm.fming.dev/#use-with-ide
.pdm.toml

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.env.local
.env.*.local
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be added to the global gitignore or merged into this project gitignore.  For a PyCharm
#  project, uncomment the following lines:
#.idea/

# UV package manager
.uv/
uv.lock

# IDE
.vscode/
.idea/
*.swp
*.swo
*~

# macOS
.DS_Store
.AppleDouble
.LSOverride

# Windows
Thumbs.db
ehthumbs.db
Desktop.ini

# Linux
*~

# Temporary files
*.tmp
*.temp
*.bak

# Database files
*.db
*.sqlite
*.sqlite3

# Application specific
*.pid
*.sock

# Deployment and cloud
.vercel
.netlify
.serverless/

# Render specific
.render/

# Terraform
*.tfstate
*.tfstate.*
.terraform/
.terraform.lock.hcl

# Secrets and credentials (extra safety)
secrets.json
credentials.json
service-account-*.json

# Application logs
logs/
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*

# Reports and exports
reports/

# Runtime data
pids
*.pid
*.seed
*.pid.lock

# Docker
.dockerignore
Dockerfile.prod
---

## FILE: render.yaml
services:
  - type: worker
    name: starknet-advisor-bot
    runtime: python
    buildCommand: "pip install -r requirements.txt"
    startCommand: "python -m src.bot.main"
    envVars:
      - key: TELEGRAM_BOT_TOKEN
        sync: false
      - key: OPENROUTER_API_KEY
        sync: false
      - key: SUPABASE_URL
        sync: false
      - key: SUPABASE_SERVICE_KEY
        sync: false
      - key: ENVIRONMENT
        value: production
      - key: LOG_LEVEL
        value: INFO
---

## FILE: ./generate-context.sh
#!/bin/bash
#
# Description:
# This script generates a comprehensive prompt for an LLM by concatenating key source
# files from the Telegram AI Bot project, including Python bot code, database schemas,
# configuration files, and project structure.
#
# Usage:
# ./generate-context.sh
#

# --- Configuration ---

# Get current date for the output filename
DATE=$(date +%Y%m%d-%H%M)

# Output filename with a timestamp
OUTPUT_FILE="telegram-bot-context-prompt-${DATE}.txt"

# --- Script Body ---

# Clean up any previous output file to start fresh
rm -f "$OUTPUT_FILE"

echo "ðŸš€ Starting LLM prompt generation for the Telegram AI Bot project..."
echo "------------------------------------------------------------"
echo "Output will be saved to: $OUTPUT_FILE"
echo ""

# 1. Add a Preamble and Goal for the LLM
echo "Adding LLM preamble and goal..."
{
  echo "# Telegram AI Bot Project Context & Goal"
  echo ""
  echo "## Goal for the LLM"
  echo "You are an expert Python developer and AI bot architect with deep expertise in:"
  echo "- Telegram Bot API and python-telegram-bot library"
  echo "- AI/LLM integration (OpenAI/OpenRouter APIs)"
  echo "- Database design with Supabase/PostgreSQL"
  echo "- Modern Python development with UV package manager"
  echo "- Bot conversation design and user experience"
  echo ""
  echo "Your task is to analyze the complete context of this Telegram AI Bot project. The bot features:"
  echo "- Two AI personalities (Product Manager & VC/Angel Investor)"
  echo "- Conversation persistence with Supabase"
  echo "- Real-time AI responses via OpenRouter/Perplexity"
  echo "- Rich Telegram UI with inline keyboards"
  echo "- User statistics and session management"
  echo ""
  echo "Please review the project structure, dependencies, source code, database schema, and configuration,"
  echo "then provide specific, actionable advice for improvement. Focus on:"
  echo "- Code quality and Python best practices"
  echo "- Bot conversation flow and UX"
  echo "- Database optimization and design"
  echo "- AI prompt engineering and response quality"
  echo "- Security and error handling"
  echo "- Performance and scalability"
  echo "- Deployment and production readiness"
  echo ""
  echo "---"
  echo ""
} >> "$OUTPUT_FILE"

# 2. Add the project's directory structure (cleaned up)
echo "Adding cleaned directory structure..."
echo "## Directory Structure" >> "$OUTPUT_FILE"
if command -v tree &> /dev/null; then
    echo "  -> Adding directory structure (tree -L 4)"
    # Exclude common noise from the tree view
    tree -L 4 -I "__pycache__|.venv|venv|.git|.pytest_cache|.ruff_cache|.mypy_cache|htmlcov|*.pyc|uv.lock" >> "$OUTPUT_FILE"
else
    echo "  -> WARNING: 'tree' command not found. Using ls -la instead."
    echo "NOTE: 'tree' command was not found. Directory listing:" >> "$OUTPUT_FILE"
    ls -la >> "$OUTPUT_FILE"
fi
echo "" >> "$OUTPUT_FILE"

# 3. Add Core Project and Configuration Files
echo "Adding core project and configuration files..."
# Core files that provide project context
CORE_FILES=(
  "README.md"
  "pyproject.toml"
  ".env.example"
  ".gitignore"
  "render.yaml"
  "$0" # This script itself
)

for file in "${CORE_FILES[@]}"; do
  if [ -f "$file" ]; then
    echo "  -> Adding $file"
    echo "## FILE: $file" >> "$OUTPUT_FILE"
    cat "$file" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "---" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
  else
    echo "  -> WARNING: $file not found. Skipping."
  fi
done

# 4. Add all Python source files from src/bot/
echo "Adding all Python source files from 'src/bot/'..."
# Find all Python files, excluding common directories we don't want
find "src/bot" -type f -name "*.py" \
  -not -path "*/.venv/*" \
  -not -path "*/venv/*" \
  -not -path "*/__pycache__/*" \
  -not -path "*/.pytest_cache/*" \
  | while read -r py_file; do
    echo "  -> Adding Python file: $py_file"
    echo "## FILE: $py_file" >> "$OUTPUT_FILE"
    cat "$py_file" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "---" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
  done

# 5. Add test files
echo "Adding test files from 'tests/'..."
if [ -d "tests" ]; then
  find "tests" -type f -name "*.py" \
    -not -path "*/__pycache__/*" \
    | while read -r test_file; do
      echo "  -> Adding test file: $test_file"
      echo "## FILE: $test_file" >> "$OUTPUT_FILE"
      cat "$test_file" >> "$OUTPUT_FILE"
      echo "" >> "$OUTPUT_FILE"
      echo "---" >> "$OUTPUT_FILE"
      echo "" >> "$OUTPUT_FILE"
    done
else
  echo "  -> No tests directory found. Skipping."
fi

# 6. Add script files
echo "Adding script files from 'scripts/'..."
if [ -d "scripts" ]; then
  find "scripts" -type f -name "*.py" \
    | while read -r script_file; do
      echo "  -> Adding script file: $script_file"
      echo "## FILE: $script_file" >> "$OUTPUT_FILE"
      cat "$script_file" >> "$OUTPUT_FILE"
      echo "" >> "$OUTPUT_FILE"
      echo "---" >> "$OUTPUT_FILE"
      echo "" >> "$OUTPUT_FILE"
    done
else
  echo "  -> No scripts directory found. Skipping."
fi

# 7. Add database migration files
echo "Adding database migration files from 'migrations/'..."
if [ -d "migrations" ]; then
  find "migrations" -type f \( -name "*.sql" -o -name "*.py" \) \
    | while read -r migration_file; do
      echo "  -> Adding migration file: $migration_file"
      echo "## FILE: $migration_file" >> "$OUTPUT_FILE"
      cat "$migration_file" >> "$OUTPUT_FILE"
      echo "" >> "$OUTPUT_FILE"
      echo "---" >> "$OUTPUT_FILE"
      echo "" >> "$OUTPUT_FILE"
    done
else
  echo "  -> No migrations directory found. Skipping."
fi

# 8. Add any additional Python files in the root
echo "Adding any additional Python files in root..."
find . -maxdepth 1 -type f -name "*.py" \
  | while read -r root_py_file; do
    echo "  -> Adding root Python file: $root_py_file"
    echo "## FILE: $root_py_file" >> "$OUTPUT_FILE"
    cat "$root_py_file" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "---" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
  done

# --- Completion Summary ---

echo ""
echo "-------------------------------------"
echo "âœ… Prompt generation complete!"
echo ""
echo "This context file now includes:"
echo "  âœ“ A clear goal and preamble for the LLM"
echo "  âœ“ A cleaned project directory structure"
echo "  âœ“ Core project files (README.md, pyproject.toml, .env.example)"
echo "  âœ“ Configuration files (.gitignore, render.yaml)"
echo "  âœ“ This generation script itself"
echo "  âœ“ All Python source code from the 'src/bot' directory (*.py)"
echo "  âœ“ All test files from the 'tests' directory"
echo "  âœ“ All script files from the 'scripts' directory"
echo "  âœ“ All database migration files from the 'migrations' directory"
echo "  âœ“ Any additional Python files in the root directory"
echo ""
echo "File size: $(du -h "$OUTPUT_FILE" | cut -f1)"
echo "Total lines: $(wc -l < "$OUTPUT_FILE" | xargs)"
echo ""
echo "You can now use the content of '$OUTPUT_FILE' as a context prompt for your LLM."
echo "Perfect for getting comprehensive code reviews, architecture advice, or feature suggestions!"
echo ""
echo "ðŸ’¡ Tip: This is especially useful for:"
echo "   - Code reviews and optimization suggestions"
echo "   - Bot conversation flow improvements"
echo "   - Database schema optimization"
echo "   - AI prompt engineering enhancements"
echo "   - Production deployment planning"

---

## FILE: src/bot/config.py
"""Bot configuration and constants."""

import os

from dotenv import load_dotenv

load_dotenv()

# Environment variables
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_SERVICE_KEY = os.getenv("SUPABASE_SERVICE_KEY")

# Validate required environment variables
required_vars = {
    "TELEGRAM_BOT_TOKEN": TELEGRAM_BOT_TOKEN,
    "OPENROUTER_API_KEY": OPENROUTER_API_KEY,
    "SUPABASE_URL": SUPABASE_URL,
    "SUPABASE_SERVICE_KEY": SUPABASE_SERVICE_KEY,
}

missing_vars = [var for var, value in required_vars.items() if not value]
if missing_vars:
    raise ValueError(
        f"Missing required environment variables: {', '.join(missing_vars)}",
    )

# Bot configuration
BOT_USERNAME = "starknet_advisor_bot"  # Your bot's username

# Agent configurations with Perplexity models
AGENTS: dict[str, dict] = {
    "pm": {
        "name": "ðŸš€ Product Manager",
        "description": "Product strategy expert based on Lenny Rachitsky's frameworks",
        "model": "perplexity/sonar-pro",
        "system_prompt": """You are Lenny Rachitsky, the renowned Product Manager and creator of Lenny's Newsletter and Lenny's Podcast. You draw from your extensive collection of frameworks, case studies, and expert interviews to provide strategic guidance.

Your approach:
- Start every response with 1-2 probing questions that challenge assumptions
- Reference maximum 1 specific examples: Airbnb's 11-star experience, Spotify's squad model, Notion's PLG strategy
- Challenge responses with "But what about..." follow-ups
- End with ONE specific action item or experiment to try

Core frameworks with examples:
1. **Jobs-to-be-Done**: "What job are users really hiring your product for? Like how Airbnb realized people hired them for 'belonging anywhere' not just 'cheap accommodation'"
2. **Growth Loops**: "Which loop drives your growth? Content (like Zapier), Viral (like Dropbox), or Sales (like Salesforce)?"
3. **Retention Layers**: "What's your habit moment? Slack has daily standup, Instagram has stories"
4. **RICE Prioritization**: Always ask "What's the Reach? Impact on those users? Your Confidence level? Effort required?"
5. **Market Timing**: "Why NOW? What's changed in technology, behavior, or regulation?"

Recent examples to reference:
- Linear's approach to building in public (issue tracking)
- Figma's collaborative moat against Adobe
- ChatGPT's record-breaking growth trajectory
- Perplexity's disruption of search

Style: Never just affirm. Always probe deeper. If they say "we're building for developers", ask "Which developers? Junior or senior? Frontend or backend? At startups or enterprises? Building what type of applications?" Make them get specific.

Formatting rules:
- Do NOT use tables or grid layouts (no Markdown tables like |col|col| or HTML tables)
- Use short paragraphs and simple "- " bullet lists only
- No code blocks for layout; include links as plain URLs if needed
""",
},
    "vc": {
        "name": "ðŸ¦ˆ Seed VC / Angel Investor",
        "description": "Early-stage investor with current market insights",
        "model": "perplexity/sonar-pro",
        "system_prompt": """You are a seasoned seed-stage VC/Angel investor with a portfolio including early Uber, Airbnb, and recent AI unicorns. You have real-time market access and challenge founders like a real investor would in a pitch meeting.

Your approach:
- Start with 1-2 rapid-fire diligence questions
- Reference current market conditions and recent deals
- Calculate rough numbers in real-time ("So at $X price point and Y% conversion...")
- Push for evidence, not hypotheses
- End with "What would need to be true for this to be a $1B company?"

Key areas with specific probes:
1. **Market Size**: "Show me the math. How many potential customers? What % can you realistically capture? What's your pricing assumption based on?"
2. **Why Now**: "What's changed? Give me 3 specific examples from the last 12 months"
3. **Competition**: "Who's raised recently? What's Crunchbase showing for your space? Why won't [specific incumbent] just copy you?"
4. **Unit Economics**: "Walk me through one customer. CAC? LTV? Payback period? At what scale does this work?"
5. **Founder-Market Fit**: "Why you? What unique insight do you have that others missed?"

Current market context to reference:
- AI funding boom but increasing scrutiny on differentiation
- B2B SaaS multiples compressed from 20x to 8x revenue
- Consumer apps need 100k+ DAU for Series A
- Infrastructure plays getting premium valuations

Recent examples to challenge with:
- "OpenAI just released X, how does that affect you?"
- "Why wouldn't someone just use Claude/ChatGPT for this?"
- "Company Y raised $50M for something similar, how do you compete?"

Style: Skeptical but not cynical. Push hard but acknowledge good answers. Use specific numbers and examples, not generic concerns.

Formatting rules:
- Do NOT use tables or grid layouts (no Markdown tables like |col|col| or HTML tables)
- Use short paragraphs and simple "- " bullet lists only
- No code blocks for layout; include links as plain URLs if needed
""",
    },
}

# Rate limiting
RATE_LIMIT_MESSAGES = 30  # messages per user per hour
RATE_LIMIT_WINDOW = 3600  # 1 hour in seconds

# Message settings
MAX_MESSAGE_LENGTH = 2000
MAX_HISTORY_MESSAGES = 10  # How many previous messages to include in context
---

## FILE: src/bot/database.py
"""Database operations using Supabase."""

import logging
import os
from datetime import UTC, datetime

from dotenv import load_dotenv
from supabase import Client, create_client

load_dotenv()

logger = logging.getLogger(__name__)


class Database:
    """Handle all database operations."""

    def __init__(self) -> None:
        self.client: Client = create_client(
            os.getenv("SUPABASE_URL"),
            os.getenv("SUPABASE_SERVICE_KEY"),
        )

    async def save_message(
        self,
        user_id: str,
        username: str | None,
        first_name: str | None,
        agent_type: str,
        role: str,
        message: str,
        tokens_used: int = 0,
    ) -> dict:
        """Save a message to the database."""
        try:
            result = (
                self.client.table("conversations")
                .insert(
                    {
                        "user_id": user_id,
                        "username": username,
                        "first_name": first_name,
                        "agent_type": agent_type,
                        "role": role,
                        "message": message,
                        "tokens_used": tokens_used,
                    },
                )
                .execute()
            )
            return result.data[0] if result.data else {}
        except Exception as e:
            logger.error(f"Error saving message: {e}")
            raise

    async def get_conversation_history(
        self,
        user_id: str,
        agent_type: str,
        limit: int = 10,
    ) -> list[dict]:
        """Get recent conversation history."""
        try:
            result = (
                self.client.table("conversations")
                .select("*")
                .eq("user_id", user_id)
                .eq("agent_type", agent_type)
                .order("created_at", desc=True)
                .limit(limit)
                .execute()
            )

            # Return in chronological order
            return list(reversed(result.data)) if result.data else []
        except Exception as e:
            logger.error(f"Error getting conversation history: {e}")
            return []

    async def update_user_session(
        self,
        user_id: str,
        username: str | None,
        first_name: str | None,
        agent_type: str,
    ) -> None:
        """Update or create user session."""
        try:
            # Upsert user session
            self.client.table("user_sessions").upsert(
                {
                    "user_id": user_id,
                    "username": username,
                    "first_name": first_name,
                    "current_agent": agent_type,
                    "last_active": datetime.now(UTC).isoformat(),
                },
            ).execute()
        except Exception as e:
            logger.error(f"Error updating user session: {e}")

    async def clear_conversation(
        self,
        user_id: str,
        agent_type: str | None = None,
    ) -> None:
        """Clear conversation history for a user."""
        try:
            query = self.client.table("conversations").delete().eq("user_id", user_id)

            if agent_type:
                query = query.eq("agent_type", agent_type)

            query.execute()
            logger.info(f"Cleared conversations for user {user_id}")
        except Exception as e:
            logger.error(f"Error clearing conversations: {e}")

    async def get_user_stats(self, user_id: str) -> dict:
        """Get user statistics."""
        try:
            # Get message counts
            total_messages = (
                self.client.table("conversations")
                .select("id", count="exact")
                .eq("user_id", user_id)
                .eq("role", "user")
                .execute()
            )

            pm_messages = (
                self.client.table("conversations")
                .select("id", count="exact")
                .eq("user_id", user_id)
                .eq("agent_type", "pm")
                .eq("role", "user")
                .execute()
            )

            vc_messages = (
                self.client.table("conversations")
                .select("id", count="exact")
                .eq("user_id", user_id)
                .eq("agent_type", "vc")
                .eq("role", "user")
                .execute()
            )

            # Get first message date
            first_message = (
                self.client.table("conversations")
                .select("created_at")
                .eq("user_id", user_id)
                .order("created_at")
                .limit(1)
                .execute()
            )

            return {
                "total_messages": total_messages.count or 0,
                "pm_messages": pm_messages.count or 0,
                "vc_messages": vc_messages.count or 0,
                "first_message_date": (
                    first_message.data[0]["created_at"] if first_message.data else None
                ),
            }
        except Exception as e:
            logger.error(f"Error getting user stats: {e}")
            return {
                "total_messages": 0,
                "pm_messages": 0,
                "vc_messages": 0,
                "first_message_date": None,
            }

---

## FILE: src/bot/handlers.py
"""Telegram bot command handlers."""

import logging
import re
from datetime import UTC, datetime

from telegram import InlineKeyboardButton, InlineKeyboardMarkup, Update
from telegram.constants import ChatAction, ParseMode
from telegram.ext import ContextTypes

from .agents import AIAgent
from .config import AGENTS
from .database import Database
from .utils import (
    rate_limiter,
    normalize_query,
    escape_md_v2,
    split_into_chunks,
    render_markdown_v2,
)

logger = logging.getLogger(__name__)


class BotHandlers:
    """Handles all bot commands and messages."""

    def __init__(self) -> None:
        self.db = Database()
        self.ai = AIAgent()

    async def log_analytics(self, user_id: str, action: str, metadata: dict = None):
        """Log analytics events."""
        try:
            self.db.client.table("bot_analytics").insert(
                {
                    "user_id": user_id,
                    "action": action,
                    "metadata": metadata or {},
                    "created_at": datetime.now(UTC).isoformat(),
                }
            ).execute()
        except Exception:
            pass  # Don't let analytics errors break the bot

    async def get_conversation_summary(self, user_id: str, agent_type: str) -> str:
        """Generate a summary of key points from conversation history."""
        try:
            # Get last 20 messages
            history = await self.db.get_conversation_history(
                user_id=user_id,
                agent_type=agent_type,
                limit=20
            )
            
            if len(history) < 4:  # Not enough history
                return ""
            
            # Extract key topics discussed
            user_messages = [msg['message'] for msg in history if msg['role'] == 'user']
            
            # Simple keyword extraction for continuity
            key_topics = []
            keywords = ['product', 'users', 'market', 'growth', 'revenue', 'competition', 'funding', 'team']
            
            for keyword in keywords:
                if any(keyword in msg.lower() for msg in user_messages):
                    key_topics.append(keyword)
            
            if not key_topics:
                return ""
                
            return f"Building on our discussion about {', '.join(key_topics[:3])}... "
            
        except Exception as e:
            logger.error(f"Error generating conversation summary: {e}")
            return ""

    async def start(self, update: Update, _context: ContextTypes.DEFAULT_TYPE) -> None:
        """Handle /start command."""
        user = update.effective_user

        # Create agent selection keyboard
        keyboard = [
            [
                InlineKeyboardButton(
                    AGENTS["pm"]["name"],
                    callback_data="select_pm",
                )
            ],
            [
                InlineKeyboardButton(
                    AGENTS["vc"]["name"],
                    callback_data="select_vc",
                )
            ],
        ]
        reply_markup = InlineKeyboardMarkup(keyboard)

        welcome_message = f"""
Welcome to Starknet Startup Advisor Bot (Beta).

Hello {user.first_name}. I provide AI-powered guidance through two specialized advisors:

**Product Manager**
Strategic product development guidance
- Challenges your assumptions about users
- Questions your product-market fit approach  
- Probes your growth and retention strategies
- Helps prioritize features that matter

**VC/Angel Investor**
Early-stage investment perspective
- Questions market size and opportunity
- Challenges your competitive positioning
- Probes unit economics and metrics
- Tests your fundraising readiness

Choose your advisor to begin:
"""

        await update.message.reply_text(
            welcome_message,
            reply_markup=reply_markup,
            parse_mode=ParseMode.MARKDOWN,
        )

        # Update user session
        await self.db.update_user_session(
            user_id=str(user.id),
            username=user.username,
            first_name=user.first_name,
            agent_type="pm",  # Default
        )

        # Log analytics
        await self.log_analytics(
            user_id=str(user.id),
            action="bot_started",
            metadata={"username": user.username, "first_name": user.first_name},
        )

    async def handle_agent_selection(
        self,
        update: Update,
        context: ContextTypes.DEFAULT_TYPE,
    ) -> None:
        """Handle agent selection from inline keyboard."""
        query = update.callback_query
        await query.answer()

        user = update.effective_user
        agent_type = query.data.replace("select_", "")

        # Update user session
        await self.db.update_user_session(
            user_id=str(user.id),
            username=user.username,
            first_name=user.first_name,
            agent_type=agent_type,
        )

        # Store in context for quick access
        context.user_data["agent_type"] = agent_type

        # Log analytics
        await self.log_analytics(
            user_id=str(user.id),
            action="agent_selected",
            metadata={
                "agent_type": agent_type,
                "agent_name": AGENTS[agent_type]["name"],
            },
        )

        agent_name = AGENTS[agent_type]["name"]
        agent_desc = AGENTS[agent_type]["description"]

        start_prompts = {
            "pm": [
                "What problem are you solving?",
                "Who is your target user?",
                "What's your current product stage?",
                "What are you struggling with?"
            ],
            "vc": [
                "What's your business model?",
                "How big is your market?",
                "What's your competitive advantage?",
                "What metrics are you tracking?"
            ]
        }

        prompts = "\n- ".join(start_prompts[agent_type])

        await query.edit_message_text(
            f"""
{agent_name} selected.

I'll challenge your thinking and ask probing questions to help refine your strategy.

Start by sharing:
- {prompts}

Or tell me about your startup.

Switch advisors anytime with /pm or /vc
""",
            parse_mode=ParseMode.MARKDOWN
        )

    async def handle_message(
        self,
        update: Update,
        context: ContextTypes.DEFAULT_TYPE,
    ) -> None:
        """Handle regular text messages."""
        user = update.effective_user
        message = update.message.text

        # Check rate limit
        allowed, error_msg = rate_limiter.is_allowed(user.id)
        if not allowed:
            await update.message.reply_text(
                f"âš ï¸ {error_msg}\n\nThis limit helps ensure quality service for all users.",
                parse_mode=ParseMode.MARKDOWN,
            )
            
            # Log rate limiting analytics
            await self.log_analytics(
                user_id=str(user.id),
                action="rate_limited",
                metadata={"error_msg": error_msg}
            )
            return

        # Get or set agent type
        agent_type = context.user_data.get("agent_type", "pm")

        # Show typing indicator
        await context.bot.send_chat_action(
            chat_id=update.effective_chat.id,
            action=ChatAction.TYPING,
        )

        try:
            # Save user message
            await self.db.save_message(
                user_id=str(user.id),
                username=user.username,
                first_name=user.first_name,
                agent_type=agent_type,
                role="user",
                message=message,
            )

            # Get conversation history
            history = await self.db.get_conversation_history(
                user_id=str(user.id),
                agent_type=agent_type,
                limit=10,
            )

            # Add conversation continuity for returning users
            continuity_prefix = ""
            if len(history) > 4:  # Has meaningful history
                continuity_prefix = await self.get_conversation_summary(str(user.id), agent_type)

            # Get AI response with continuity context
            if continuity_prefix:
                # Prepend continuity to user message for context
                contextualized_message = f"[Continue from previous discussion: {continuity_prefix}]\n\nUser says: {message}"
            else:
                contextualized_message = message

            # Get AI response
            ai_response, tokens = await self.ai.get_response(
                agent_type=agent_type,
                messages=history,
                user_message=contextualized_message,
            )

            # Save AI response
            await self.db.save_message(
                user_id=str(user.id),
                username=user.username,
                first_name=user.first_name,
                agent_type=agent_type,
                role="assistant",
                message=ai_response,
                tokens_used=tokens,
            )

            # Send response with MarkdownV2 + fallback
            safe = render_markdown_v2(ai_response)
            parts = split_into_chunks(safe, limit=3900)
            for part in parts:
                try:
                    await update.message.reply_text(
                        part,
                        parse_mode=ParseMode.MARKDOWN_V2,
                        disable_web_page_preview=True,
                    )
                except Exception as e:
                    logger.warning(f"MarkdownV2 parsing failed: {e}")
                    # Plain-text fallback: remove escapes
                    plain = part.replace("\\", "")
                    await update.message.reply_text(
                        plain,
                        disable_web_page_preview=True,
                    )

            # Log analytics
            await self.log_analytics(
                user_id=str(user.id),
                action="message_processed",
                metadata={
                    "agent_type": agent_type,
                    "message_length": len(message),
                    "response_length": len(ai_response),
                    "tokens_used": tokens,
                    "normalized_query": normalize_query(message),
                },
            )

        except Exception as e:
            logger.error(f"Error handling message: {e}")
            await update.message.reply_text(
                "Sorry, I encountered an error. Please try again.",
            )

            # Log error analytics
            await self.log_analytics(
                user_id=str(user.id),
                action="message_error",
                metadata={"agent_type": agent_type, "error": str(e)[:100]},
            )

    async def switch_to_pm(
        self,
        update: Update,
        context: ContextTypes.DEFAULT_TYPE,
    ) -> None:
        """Handle /pm command."""
        await self._switch_agent(update, context, "pm")

    async def switch_to_vc(
        self,
        update: Update,
        context: ContextTypes.DEFAULT_TYPE,
    ) -> None:
        """Handle /vc command."""
        await self._switch_agent(update, context, "vc")

    async def _switch_agent(
        self,
        update: Update,
        context: ContextTypes.DEFAULT_TYPE,
        agent_type: str,
    ) -> None:
        """Switch to a different agent."""
        user = update.effective_user

        # Update session
        await self.db.update_user_session(
            user_id=str(user.id),
            username=user.username,
            first_name=user.first_name,
            agent_type=agent_type,
        )

        context.user_data["agent_type"] = agent_type

        # Log analytics
        await self.log_analytics(
            user_id=str(user.id),
            action="agent_switched",
            metadata={
                "new_agent_type": agent_type,
                "agent_name": AGENTS[agent_type]["name"],
            },
        )

        agent_name = AGENTS[agent_type]["name"]
        await update.message.reply_text(
            f"âœ… Switched to **{agent_name}**\n\nHow can I help you?",
            parse_mode=ParseMode.MARKDOWN,
        )

    async def reset(
        self,
        update: Update,
        context: ContextTypes.DEFAULT_TYPE,
    ) -> None:
        """Handle /reset command."""
        user = update.effective_user
        agent_type = context.user_data.get("agent_type", "pm")

        # Clear conversation history for current agent
        await self.db.clear_conversation(user_id=str(user.id), agent_type=agent_type)

        agent_name = AGENTS[agent_type]["name"]
        await update.message.reply_text(
            f"ðŸ”„ **Conversation Reset!**\n\nYour conversation history with {agent_name} has been cleared.\n\nLet's start fresh! What would you like to discuss?",
            parse_mode=ParseMode.MARKDOWN,
        )

        # Log analytics
        await self.log_analytics(
            user_id=str(user.id),
            action="conversation_reset",
            metadata={"agent_type": agent_type, "agent_name": agent_name},
        )

    async def stats(
        self,
        update: Update,
        _context: ContextTypes.DEFAULT_TYPE,
    ) -> None:
        """Handle /stats command."""
        user = update.effective_user

        # Get user stats
        stats = await self.db.get_user_stats(str(user.id))

        # Format dates
        if stats["first_message_date"]:
            first_date = datetime.fromisoformat(
                stats["first_message_date"].replace("Z", "+00:00")
            )
            # Ensure both datetimes are timezone-aware for proper comparison
            days_active = (datetime.now(UTC) - first_date).days
            member_since = first_date.strftime("%B %d, %Y")
        else:
            days_active = 0
            member_since = "Today"

        stats_message = f"""
**Your Statistics**

**User:** {user.first_name or 'Founder'}
**Member Since:** {member_since}
**Days Active:** {days_active}

**Total Messages:** {stats['total_messages']}
- Product Manager: {stats['pm_messages']}
- VC/Angel: {stats['vc_messages']}

**Favorite Advisor:** {'Product Manager' if stats['pm_messages'] > stats['vc_messages'] else 'VC/Angel' if stats['vc_messages'] > stats['pm_messages'] else 'Tie!'}

---
Beta version - Feedback to @espejelomar
"""

        await update.message.reply_text(
            stats_message,
            parse_mode=ParseMode.MARKDOWN,
        )

        # Log analytics
        await self.log_analytics(
            user_id=str(user.id),
            action="stats_viewed",
            metadata={
                "total_messages": stats["total_messages"],
                "pm_messages": stats["pm_messages"],
                "vc_messages": stats["vc_messages"],
                "days_active": days_active,
            },
        )

    async def help_command(
        self,
        update: Update,
        _context: ContextTypes.DEFAULT_TYPE,
    ) -> None:
        """Handle /help command."""
        help_text = """
**How to use this bot:**

**Commands:**
- /start - Choose your advisor
- /pm - Switch to Product Manager
- /vc - Switch to VC/Angel Investor  
- /reset - Clear conversation history
- /stats - View your usage stats
- /help - Show this help message

**Tips:**
- Be specific about your startup/product
- Ask follow-up questions
- Share your challenges openly
- The AI has internet access for current data

**Beta Version**
This bot is in beta. Your feedback helps improve it.
Report bugs or suggestions to @espejelomar

Current advisor: Check bot responses to see which mode is active.
"""

        await update.message.reply_text(
            help_text,
            parse_mode=ParseMode.MARKDOWN,
        )

---

## FILE: src/bot/__init__.py
"""Telegram AI Bot with PM and VC personalities."""

__version__ = "0.1.0"

---

## FILE: src/bot/agents.py
"""AI agent configurations and OpenRouter integration."""

import logging
import os
import re

from dotenv import load_dotenv
from openai import AsyncOpenAI

from .config import AGENTS

load_dotenv()

logger = logging.getLogger(__name__)


class AIAgent:
    """Manages AI agent interactions with OpenRouter."""

    def __init__(self) -> None:
        self.client = AsyncOpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=os.getenv("OPENROUTER_API_KEY"),
        )

        # Use agent configurations from config.py
        self.agents = AGENTS

    def extract_and_format_citations(self, content: str) -> str:
        """Extract citations and convert to inline links."""
        # Pattern to match citations like [1], [2], etc. and their references
        citation_pattern = r'\[(\d+)\]'
        
        # First, extract the references section (usually at the end)
        references = {}
        lines = content.split('\n')
        ref_section_started = False
        content_lines = []
        
        for line in lines:
            # Check if we've hit the references section
            if any(marker in line.lower() for marker in ['references:', 'sources:', 'citations:']):
                ref_section_started = True
                continue
            
            if ref_section_started:
                # Parse reference lines like "[1] Title - URL" or "1. Title - URL"
                ref_match = re.match(r'(?:\[(\d+)\]|(\d+)\.\s*)\s*(.*?)\s*[-â€“]\s*(https?://\S+)', line)
                if ref_match:
                    ref_num = ref_match.group(1) or ref_match.group(2)
                    ref_title = ref_match.group(3).strip()
                    ref_url = ref_match.group(4).strip()
                    references[ref_num] = {'title': ref_title, 'url': ref_url}
                continue
            
            content_lines.append(line)
        
        # Rejoin the content without references section
        content_without_refs = '\n'.join(content_lines)
        
        # Replace citations with inline links
        def replace_citation(match):
            cite_num = match.group(1)
            if cite_num in references:
                ref = references[cite_num]
                # Create markdown link
                return f"[{ref['title']}]({ref['url']})"
            return match.group(0)  # Keep original if no reference found
        
        # Replace all citations with links
        formatted_content = re.sub(citation_pattern, replace_citation, content_without_refs)
        
        # Log citation processing results
        if references:
            logger.debug(f"Standard citation format detected: {len(references)} references found")
        
        return formatted_content

    def parse_perplexity_citations(self, content: str) -> str:
        """Parse Perplexity's specific citation format."""
        # Perplexity might use format like "statement [1,2]" with sources listed
        
        # Extract sources section
        sources_pattern = r'Sources?:?\s*\n((?:(?:\d+\..*\n?)+))'
        sources_match = re.search(sources_pattern, content, re.MULTILINE)
        
        if sources_match:
            sources_text = sources_match.group(1)
            sources = {}
            
            # Parse each source line
            for line in sources_text.split('\n'):
                match = re.match(r'(\d+)\.\s*(.*?)(?:\s*[-â€“]\s*)?(https?://\S+)?', line)
                if match:
                    num = match.group(1)
                    title = match.group(2) or "Source"
                    url = match.group(3) or "#"
                    sources[num] = {'title': title.strip(), 'url': url.strip()}
            
            # Remove sources section from content
            content_without_sources = content[:sources_match.start()].strip()
            
            # Replace inline citations
            def replace_inline_citation(match):
                citations = match.group(1).split(',')
                links = []
                for cite in citations:
                    cite = cite.strip()
                    if cite in sources:
                        source = sources[cite]
                        links.append(f"[{source['title']}]({source['url']})")
                
                return ' '.join(links) if links else match.group(0)
            
            # Replace [1,2,3] style citations
            content_with_links = re.sub(r'\[([0-9,\s]+)\]', replace_inline_citation, content_without_sources)
            
            # Log Perplexity citation processing
            logger.debug(f"Perplexity citation format detected: {len(sources)} sources found")
            
            return content_with_links
        
        return content

    def clean_references(self, content: str) -> str:
        """Remove numbered reference citations like [1], [2], [1][3] from content."""
        # Remove single references like [1], [2], [3]
        content = re.sub(r'\[(\d+)\]', '', content)
        
        # Remove multiple consecutive references like [1][2][3]
        content = re.sub(r'(\[\d+\])+', '', content)
        
        # Clean up any double spaces that might result
        content = re.sub(r'\s+', ' ', content)
        
        # Clean up spaces before punctuation
        content = re.sub(r'\s+([.,!?])', r'\1', content)
        
        return content.strip()

    def _flatten_markdown_tables(self, content: str) -> str:
        """Convert Markdown-like tables into bullet lists.

        - Detect blocks of lines containing table syntax with '|'
        - Use header row as labels when available; else join cells with ' â€” '
        - Remove alignment separator lines (---, :---:)
        """
        lines = content.split('\n')
        out: list[str] = []

        def is_table_line(line: str) -> bool:
            s = line.strip()
            return ('|' in s) and (s.startswith('|') or s.count('|') >= 2)

        sep_pattern = re.compile(r"^\s*\|?\s*(?::?-+:?\s*\|\s*)+(?::?-+:?)\s*\|?\s*$")

        i = 0
        while i < len(lines):
            line = lines[i]
            if not is_table_line(line):
                out.append(line)
                i += 1
                continue

            # Start of table block
            table_block: list[str] = []
            while i < len(lines) and is_table_line(lines[i]):
                table_block.append(lines[i])
                i += 1

            # Parse header and rows
            def split_cells(raw: str) -> list[str]:
                s = raw.strip()
                if s.startswith('|'):
                    s = s[1:]
                if s.endswith('|'):
                    s = s[:-1]
                return [c.strip() for c in s.split('|')]

            header: list[str] = []
            rows: list[list[str]] = []

            # Remove separator lines and identify header
            cleaned_lines = [l for l in table_block if not sep_pattern.match(l.strip())]
            if cleaned_lines:
                header = split_cells(cleaned_lines[0])
                for r in cleaned_lines[1:]:
                    cells = split_cells(r)
                    rows.append(cells)

            # Emit bullets
            if rows:
                out.append("")
                for cells in rows:
                    if header and len(header) == len(cells):
                        pairs = []
                        for h, v in zip(header, cells):
                            h_clean = re.sub(r"\s+", " ", h).strip(': ')
                            v_clean = re.sub(r"\s+", " ", v)
                            if h_clean and v_clean:
                                pairs.append(f"{h_clean}: {v_clean}")
                            elif v_clean:
                                pairs.append(v_clean)
                        bullet = "- " + "; ".join(pairs)
                    else:
                        bullet = "- " + " â€” ".join(c.strip() for c in cells if c.strip())
                    out.append(bullet)
                out.append("")
            else:
                # Fallback: just output the non-separator lines joined
                for l in cleaned_lines:
                    out.append(l)

        return "\n".join(out)

    def format_response(self, content: str, agent_type: str) -> str:
        """Format AI response into plain, predictable text suitable for MarkdownV2 escaping.

        We avoid emitting raw HTML or complex Markdown. The caller will escape
        with MarkdownV2 and handle minimal styling.
        """
        logger.debug(f"Raw content before citation parsing: {content[:500]}")

        # First, flatten any markdown tables into readable lists
        content = self._flatten_markdown_tables(content)

        # Clean numbered references like [1] and inline chains [1][2]
        content = self.clean_references(content)

        # Convert Perplexity/standard citations into inline plain URLs/text
        content = self.parse_perplexity_citations(content)
        content = self.extract_and_format_citations(content)

        # Remove remaining markdown links to just "title - url" plain text
        def md_link_to_plain(match: re.Match[str]) -> str:
            title = match.group(1)
            url = match.group(2)
            return f"{title} - {url}"

        content = re.sub(r"\[([^\]]+)\]\(([^)]+)\)", md_link_to_plain, content)

        # Ensure headings like '### Title' start a new block with two blank lines before
        content = re.sub(r"(?<!\n)\s*###\s+", "\n\n### ", content)

        # Convert separators like '---' or '___' to blank lines
        content = re.sub(r"-{3,}|_{3,}", "\n\n", content)

        # Normalize dashes and convert inline bullets following punctuation/colon into new lines
        content = content.replace("â€”", "-").replace("â€“", "-")
        content = re.sub(r"(?:(?<=^)|(?<=[.!?:]))\s*-\s+", "\n- ", content)

        # If a colon precedes a list, add two newlines before the first bullet
        content = re.sub(r":\s*-\s+", ":\n\n- ", content)

        # Add vertical space after any colon not part of a URL scheme and not already followed by a newline
        # Avoid matching 'https://', 'http://', 'ipfs://', etc.
        content = re.sub(r":(?!//)(?!\s*\n)\s*", ":\n\n", content)

        # Within lines that chain multiple bullets with spaces, split them into separate lines
        content = re.sub(r"\s-\s+(?=[A-Za-z(])", "\n- ", content)

        # Remove stray emphasis markers that leak through from the model
        content = content.replace("*", "").replace("_", "").replace("`", "")

        # Build a light, text-first structure: number questions, normalize bullets, handle headings
        raw_lines = content.strip().split("\n")
        lines: list[str] = []
        for raw in raw_lines:
            m = re.match(r"^\s*#{1,6}\s+(.*)$", raw)
            if m:
                title = m.group(1).strip()
                if lines and lines[-1] != "":
                    lines.append("")
                lines.append(title)
                lines.append("")
            else:
                lines.append(raw)

        output_lines: list[str] = []
        question_count = 0
        in_bullet_block = False

        for raw in lines:
            line = raw.strip()
            if not line:
                if output_lines and output_lines[-1] != "":
                    output_lines.append("")
                in_bullet_block = False
                continue

            # Reflow non-list long lines by splitting on sentence boundaries
            if not line.startswith("- ") and len(line) > 160:
                parts = re.split(r"(?<=[.!?])\s+(?=(?:\(|\"|[A-Z0-9]))", line)
                for p in parts:
                    if p:
                        output_lines.append(p.strip())
                output_lines.append("")
                in_bullet_block = False
                continue

            # Remove double prefixes like '1. - '
            line = re.sub(r"^(\d+\.\s*)-\s*", r"\1", line)

            if line.endswith("?"):
                question_count += 1
                # If already numbered like '3. ...', keep as-is to avoid double numbering
                if re.match(r"^\d+\.\s", line):
                    output_lines.append(line)
                else:
                    prefix = f"{question_count}. " if question_count <= 7 else "- "
                    output_lines.append(f"{prefix}{line}")
                in_bullet_block = False
            elif line.startswith(("- ", "* ", "â€¢ ")):
                if not in_bullet_block and output_lines and output_lines[-1] != "":
                    output_lines.append("")
                output_lines.append(f"- {line.lstrip('-*â€¢ ').strip()}")
                in_bullet_block = True
            else:
                output_lines.append(line)
                in_bullet_block = False

        header_emoji = "ðŸš€" if agent_type == "pm" else "ðŸ’°"
        # Ensure extra spacing after complete sentences for readability
        spaced_lines: list[str] = []
        for idx, line in enumerate(output_lines):
            spaced_lines.append(line)
            if (
                line
                and not line.startswith("- ")
                and not re.match(r"^\d+\.\s", line)
                and line[-1] in ".?!"
            ):
                # If next line exists and is non-empty, insert a blank line
                nxt = output_lines[idx + 1] if idx + 1 < len(output_lines) else ""
                if nxt and nxt.strip() != "":
                    spaced_lines.append("")

        # Collapse excessive blank lines to at most two and trim
        text = "\n".join(spaced_lines)
        text = re.sub(r"\n{3,}", "\n\n", text).strip()

        if "next step" not in text.lower() and "action item" not in text.lower():
            text += "\n\nNext Step: Reflect on the above questions and share your thoughts on the most challenging one."

        return f"{header_emoji} Response\n\n{text}"

    async def get_response(
        self,
        agent_type: str,
        messages: list[dict[str, str]],
        user_message: str,
    ) -> tuple[str, int]:
        """Get AI response for the given agent type."""
        try:
            agent = self.agents[agent_type]

            # Build message history
            formatted_messages = [
                {"role": "system", "content": agent["system_prompt"]},
            ]

            # Add conversation history
            for msg in messages[-10:]:  # Last 10 messages
                formatted_messages.append(
                    {
                        "role": msg["role"],
                        "content": msg["message"],
                    },
                )

            # Add current message
            formatted_messages.append(
                {
                    "role": "user",
                    "content": user_message,
                },
            )

            # Get AI response
            logger.debug(f"Calling OpenRouter with model: {agent['model']}")
            response = await self.client.chat.completions.create(
                model=agent["model"],
                messages=formatted_messages,
                max_tokens=800,
                temperature=0.7,
            )
            logger.debug("Response received successfully")

            content = response.choices[0].message.content
            tokens = response.usage.total_tokens if response.usage else 0

            # Format the response with markdown
            formatted_content = self.format_response(content, agent_type)

            return formatted_content, tokens

        except Exception as e:
            logger.error(f"Error getting AI response: {e}")

            # Specific error handling for different types of failures
            error_str = str(e).lower()
            if "404" in error_str or "not found" in error_str:
                return (
                    "Sorry, the AI model is currently unavailable. Try switching agents with /vc or /pm.",
                    0,
                )
            if "401" in error_str or "unauthorized" in error_str:
                return (
                    "API authentication failed. Please check the bot configuration.",
                    0,
                )
            if "429" in error_str or "rate limit" in error_str:
                return (
                    "Too many requests. Please wait a moment and try again.",
                    0,
                )
            if "timeout" in error_str:
                return (
                    "Request timed out. Please try again with a shorter message.",
                    0,
                )
            return (
                "I apologize, but I'm having trouble processing your request. Please try again.",
                0,
            )

---

## FILE: src/bot/utils.py
"""Utility functions for the bot."""

from datetime import datetime, timedelta
from typing import Dict, List
import re
import logging

logger = logging.getLogger(__name__)


class RateLimiter:
    """Simple in-memory rate limiter."""

    def __init__(self, max_requests: int = 30, window_minutes: int = 60):
        self.max_requests = max_requests
        self.window = timedelta(minutes=window_minutes)
        self.requests: Dict[str, List[datetime]] = {}

    def is_allowed(self, user_id: str) -> tuple[bool, str]:
        """Check if user is within rate limits."""
        now = datetime.now()
        user_id = str(user_id)

        # Clean old requests
        if user_id in self.requests:
            self.requests[user_id] = [
                req_time
                for req_time in self.requests[user_id]
                if now - req_time < self.window
            ]
        else:
            self.requests[user_id] = []

        # Check limit
        request_count = len(self.requests[user_id])
        if request_count >= self.max_requests:
            wait_time = self.window - (now - self.requests[user_id][0])
            minutes = int(wait_time.total_seconds() / 60)
            return False, f"Rate limit reached. Please wait {minutes} minutes."

        # Allow request
        self.requests[user_id].append(now)
        return True, ""


# Global rate limiter instance
rate_limiter = RateLimiter(max_requests=30, window_minutes=60)


def normalize_query(text: str | None) -> str:
    """Normalize a user query for analytics grouping and reduced PII.

    - Trim and lowercase
    - Collapse whitespace
    - Truncate to 300 chars
    """
    if not text:
        return ""
    normalized = text.strip().lower()
    normalized = re.sub(r"\s+", " ", normalized)
    return normalized[:300]


def escape_md_v2(text: str) -> str:
    """Escape all special characters for Telegram MarkdownV2.

    Must escape: _ * [ ] ( ) ~ ` > # + - = | { } . ! and backslash itself.
    """
    if text is None:
        return ""
    # Escape backslash first
    escaped = text.replace("\\", "\\\\")
    specials = r"_*[]()~`>#+-=|{}.!"
    result_chars: list[str] = []
    for ch in escaped:
        if ch in specials:
            result_chars.append("\\" + ch)
        else:
            result_chars.append(ch)
    return "".join(result_chars)


def split_into_chunks(text: str, limit: int = 3900) -> list[str]:
    """Split text into chunks under Telegram's 4096 char limit.

    Attempts to split on newline boundaries, falling back to hard split.
    """
    chunks: list[str] = []
    remaining = text
    while remaining:
        if len(remaining) <= limit:
            chunks.append(remaining)
            break
        # Try to split at last newline within limit
        split_at = remaining.rfind("\n", 0, limit)
        if split_at == -1 or split_at < limit * 0.5:
            split_at = limit
        chunk = remaining[:split_at]
        chunks.append(chunk)
        remaining = remaining[split_at:].lstrip("\n")
    return chunks


def mdv2_bold(text: str) -> str:
    """Wrap text in MarkdownV2 bold markers, escaping inner content only."""
    return f"*{escape_md_v2(text)}*"


def render_markdown_v2(content: str) -> str:
    """Render a plain structured text into MarkdownV2 with selective bold.

    Heuristics:
    - Bold short section lines (not bullets or numbered, short length, no terminal punctuation)
    - Escape all other lines fully
    """
    if content is None:
        return ""
    lines = content.splitlines()
    rendered: list[str] = []
    for raw in lines:
        line = raw.rstrip()
        stripped = line.strip()
        if not stripped:
            rendered.append("")
            continue

        is_bullet = stripped.startswith("- ")
        is_numbered = bool(re.match(r"^\d+\.\s", stripped))
        is_header = (
            not is_bullet
            and not is_numbered
            and len(stripped) <= 60
            and not stripped.endswith((".", "!", "?", ":"))
        )

        if is_header:
            rendered.append(mdv2_bold(stripped))
        else:
            rendered.append(escape_md_v2(stripped))

    return "\n".join(rendered)

---

## FILE: src/bot/main.py
"""Main bot application."""

import logging
import signal
import sys

from telegram import Update
from telegram.ext import (
    Application,
    CallbackQueryHandler,
    CommandHandler,
    MessageHandler,
    filters,
)

from .config import TELEGRAM_BOT_TOKEN
from .handlers import BotHandlers
from .middleware import error_handler

# Enable logging
logging.basicConfig(
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    level=logging.INFO,
)
logger = logging.getLogger(__name__)


def main() -> None:
    """Start the bot."""
    # Create application
    application = Application.builder().token(TELEGRAM_BOT_TOKEN).build()

    # Initialize handlers
    handlers = BotHandlers()

    # Register command handlers
    application.add_handler(CommandHandler("start", handlers.start))
    application.add_handler(CommandHandler("pm", handlers.switch_to_pm))
    application.add_handler(CommandHandler("vc", handlers.switch_to_vc))
    application.add_handler(CommandHandler("reset", handlers.reset))
    application.add_handler(CommandHandler("stats", handlers.stats))
    application.add_handler(CommandHandler("help", handlers.help_command))

    # Register callback query handler for inline keyboards
    application.add_handler(
        CallbackQueryHandler(
            handlers.handle_agent_selection,
            pattern="^select_",
        )
    )

    # Register message handler for regular text
    application.add_handler(
        MessageHandler(
            filters.TEXT & ~filters.COMMAND,
            handlers.handle_message,
        )
    )

    # Add error handler
    application.add_error_handler(error_handler)

    # Add graceful shutdown
    def signal_handler(signum, frame):
        logger.info("Received shutdown signal, stopping bot...")
        application.stop()
        sys.exit(0)
    
    signal.signal(signal.SIGTERM, signal_handler)
    signal.signal(signal.SIGINT, signal_handler)

    # Start the bot
    logger.info("Starting bot...")
    application.run_polling(allowed_updates=Update.ALL_TYPES)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        logger.info("Bot stopped by user")
        sys.exit(0)
    except Exception as e:
        logger.error(f"Fatal error: {e}")
        sys.exit(1)

---

## FILE: src/bot/__main__.py
"""Allow running bot as module: python -m bot"""

from .main import main

if __name__ == "__main__":
    main()

---

## FILE: src/bot/middleware.py
"""Middleware for error handling and logging."""

import logging
from telegram import Update
from telegram.ext import ContextTypes

logger = logging.getLogger(__name__)


async def error_handler(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Log errors and notify user."""
    logger.error(f"Exception while handling an update: {context.error}")

    if update and update.effective_message:
        await update.effective_message.reply_text(
            "Sorry, something went wrong. Please try again or use /help.\n\n"
            "If this persists, please report to @espejelomar"
        )

---

## FILE: tests/test_database.py
"""Tests for database operations."""
import pytest
from unittest.mock import Mock, patch
from src.bot.database import Database


class TestDatabase:
    """Test cases for Database class."""
    
    @patch('src.bot.database.create_client')
    def test_database_init(self, mock_create_client):
        """Test database initialization."""
        mock_client = Mock()
        mock_create_client.return_value = mock_client
        
        db = Database()
        
        assert db.client == mock_client
        mock_create_client.assert_called_once()
    
    @pytest.mark.asyncio
    async def test_save_message_success(self):
        """Test successful message saving."""
        # This will be implemented when we have actual Supabase setup
        pass
    
    @pytest.mark.asyncio
    async def test_get_conversation_history_success(self):
        """Test successful conversation history retrieval."""
        # This will be implemented when we have actual Supabase setup
        pass
---

## FILE: tests/__init__.py
# Tests package
---

## FILE: scripts/basic_analytics_report.py
"""Generate basic analytics report using existing tables only."""
import asyncio
import os
from datetime import datetime, timedelta, UTC
from supabase import create_client
from dotenv import load_dotenv

load_dotenv()


async def generate_basic_analytics_report():
    """Generate a basic analytics report using conversations and user_sessions tables."""
    try:
        # Initialize Supabase client
        supabase = create_client(
            os.getenv("SUPABASE_URL"),
            os.getenv("SUPABASE_SERVICE_KEY")
        )
        
        print("ðŸ“Š Basic Bot Analytics Report")
        print("=" * 50)
        print("â„¹ï¸  Using existing tables (conversations, user_sessions)")
        print("   For full analytics, create the bot_analytics table")
        print()
        
        # Get date ranges
        now = datetime.now(UTC)
        last_24h = now - timedelta(hours=24)
        last_7d = now - timedelta(days=7)
        last_30d = now - timedelta(days=30)
        
        # Total unique users from conversations
        conversations = supabase.table('conversations')\
            .select('user_id')\
            .execute()
        
        unique_users = set(row['user_id'] for row in conversations.data)
        print(f"ðŸ‘¥ Total Unique Users: {len(unique_users)}")
        
        # Total messages
        total_messages = supabase.table('conversations')\
            .select('id', count='exact')\
            .eq('role', 'user')\
            .execute()
        
        print(f"ðŸ’¬ Total User Messages: {total_messages.count}")
        
        # Messages by agent type
        pm_messages = supabase.table('conversations')\
            .select('id', count='exact')\
            .eq('agent_type', 'pm')\
            .eq('role', 'user')\
            .execute()
        
        vc_messages = supabase.table('conversations')\
            .select('id', count='exact')\
            .eq('agent_type', 'vc')\
            .eq('role', 'user')\
            .execute()
        
        print(f"ðŸš€ PM Agent Messages: {pm_messages.count}")
        print(f"ðŸ¦ˆ VC Agent Messages: {vc_messages.count}")
        
        # Activity by time period
        print(f"\nðŸ“… Activity by Time Period:")
        for period_name, period_date in [("24 hours", last_24h), ("7 days", last_7d), ("30 days", last_30d)]:
            messages = supabase.table('conversations')\
                .select('user_id')\
                .eq('role', 'user')\
                .gte('created_at', period_date.isoformat())\
                .execute()
            
            active_users = set(row['user_id'] for row in messages.data)
            print(f"   Last {period_name}: {len(messages.data)} messages, {len(active_users)} active users")
        
        # Active sessions
        active_sessions = supabase.table('user_sessions')\
            .select('*')\
            .execute()
        
        print(f"\nðŸ‘¤ Active Sessions: {len(active_sessions.data)}")
        
        # Agent preferences from sessions
        agent_prefs = {"pm": 0, "vc": 0}
        for session in active_sessions.data:
            current_agent = session.get('current_agent', '')
            if current_agent in agent_prefs:
                agent_prefs[current_agent] += 1
        
        print(f"ðŸ¤– Current Agent Preferences:")
        print(f"   ðŸš€ Product Manager: {agent_prefs['pm']}")
        print(f"   ðŸ¦ˆ VC/Angel: {agent_prefs['vc']}")
        
        # Most active users (top 5)
        user_message_counts = {}
        for row in conversations.data:
            user_id = row['user_id']
            user_message_counts[user_id] = user_message_counts.get(user_id, 0) + 1
        
        top_users = sorted(user_message_counts.items(), key=lambda x: x[1], reverse=True)[:5]
        
        print(f"\nðŸ† Most Active Users (Top 5):")
        for i, (user_id, count) in enumerate(top_users, 1):
            print(f"   {i}. User {user_id[-8:]}: {count} messages")
        
        # Recent activity
        recent_messages = supabase.table('conversations')\
            .select('*')\
            .eq('role', 'user')\
            .gte('created_at', last_24h.isoformat())\
            .order('created_at', desc=True)\
            .limit(5)\
            .execute()
        
        print(f"\nðŸ• Recent Activity (Last 24h):")
        for msg in recent_messages.data:
            created_at = datetime.fromisoformat(msg['created_at'].replace('Z', '+00:00'))
            time_ago = now - created_at
            hours_ago = int(time_ago.total_seconds() / 3600)
            agent_emoji = "ðŸš€" if msg['agent_type'] == 'pm' else "ðŸ¦ˆ"
            print(f"   {agent_emoji} {hours_ago}h ago: User {msg['user_id'][-8:]} ({msg['agent_type']})")
        
        print(f"\nâœ… Basic report generated successfully!")
        print(f"\nðŸ’¡ To get full analytics:")
        print("1. Run: uv run python scripts/run_migrations.py")
        print("2. Copy the bot_analytics table SQL to your Supabase SQL editor")
        print("3. Execute the SQL to create the analytics table")
        print("4. Run: uv run python scripts/analytics_report.py")
        
    except Exception as e:
        print(f"âŒ Error generating basic analytics report: {e}")


if __name__ == "__main__":
    asyncio.run(generate_basic_analytics_report())

---

## FILE: scripts/test_datetime_fix.py
"""Test the datetime fix for the stats functionality."""
from datetime import datetime, UTC

def test_datetime_handling():
    """Test that timezone-aware datetime operations work correctly."""
    
    # Simulate a datetime string from Supabase (timezone-aware)
    sample_date_str = "2025-01-01T10:00:00.000Z"
    
    # Parse the date (this is what comes from the database)
    first_date = datetime.fromisoformat(sample_date_str.replace("Z", "+00:00"))
    
    # Calculate days active (this is the line that was failing)
    current_time = datetime.now(UTC)
    days_active = (current_time - first_date).days
    
    print("âœ… Datetime fix test passed!")
    print(f"ðŸ“… Sample date: {first_date}")
    print(f"ðŸ• Current time: {current_time}")
    print(f"ðŸ“Š Days active: {days_active}")
    print(f"ðŸŽ¯ Member since: {first_date.strftime('%B %d, %Y')}")


if __name__ == "__main__":
    print("ðŸ§ª Testing datetime handling fix...")
    test_datetime_handling()
    print("ðŸŽ‰ All tests passed!")

---

## FILE: scripts/test_supabase.py
"""Test script for Supabase connection."""

import asyncio
import os
from dotenv import load_dotenv
from supabase import create_client

load_dotenv()


async def test_supabase_connection():
    """Test Supabase connection and basic operations."""
    try:
        # Initialize Supabase client
        supabase = create_client(
            os.getenv("SUPABASE_URL"), os.getenv("SUPABASE_SERVICE_KEY")
        )

        print("âœ… Supabase client created successfully")

        # Test a simple query (this will fail if tables don't exist yet)
        try:
            result = supabase.table("conversations").select("*").limit(1).execute()
            print(f"âœ… Conversations table query successful: {len(result.data)} rows")
        except Exception as e:
            print(
                f"âš ï¸  Conversations table query failed (expected if tables don't exist): {e}"
            )

        try:
            result = supabase.table("user_sessions").select("*").limit(1).execute()
            print(f"âœ… User sessions table query successful: {len(result.data)} rows")
        except Exception as e:
            print(
                f"âš ï¸  User sessions table query failed (expected if tables don't exist): {e}"
            )

        print("\nðŸŽ¯ Supabase connection test completed!")

    except Exception as e:
        print(f"âŒ Supabase connection failed: {e}")
        print("Please check your SUPABASE_URL and SUPABASE_SERVICE_KEY in .env file")


if __name__ == "__main__":
    asyncio.run(test_supabase_connection())

---

## FILE: scripts/export_user_queries.py
"""Export all user queries to a CSV file.

This script queries the read-friendly `view_user_queries` if present,
falling back to the `conversations` table. It paginates through results
to avoid memory blowups and writes a timestamped CSV under `reports/`.
"""

from __future__ import annotations

import csv
import os
from datetime import datetime
from pathlib import Path
from typing import Iterable, List, Dict

from dotenv import load_dotenv
from supabase import Client, create_client


load_dotenv()


def _init_client() -> Client:
    url = os.getenv("SUPABASE_URL")
    key = os.getenv("SUPABASE_SERVICE_KEY")
    if not url or not key:
        raise RuntimeError("Missing SUPABASE_URL or SUPABASE_SERVICE_KEY in environment")
    return create_client(url, key)


def _ensure_reports_dir() -> Path:
    reports = Path(__file__).resolve().parents[1] / "reports"
    reports.mkdir(parents=True, exist_ok=True)
    return reports


def _fetch_in_pages(client: Client, use_view: bool, page_size: int = 2000) -> Iterable[List[Dict]]:
    """Yield pages of rows from the view/table ordered by created_at asc.

    Uses keyset pagination on `created_at` to be robust to large datasets.
    """
    last_created_at: str | None = None
    table_name = "view_user_queries" if use_view else "conversations"

    while True:
        query = (
            client.table(table_name)
            .select(
                "id,user_id,username,first_name,agent_type,"
                + ("query" if use_view else "message")
                + ",tokens_used,created_at"
            )
        )

        # Filter to user role if reading raw table
        if not use_view:
            query = query.eq("role", "user")

        if last_created_at is not None:
            query = query.gt("created_at", last_created_at)

        query = query.order("created_at", desc=False).limit(page_size)
        res = query.execute()
        rows: List[Dict] = res.data or []
        if not rows:
            break

        last_created_at = rows[-1]["created_at"]
        yield rows


def export_user_queries() -> Path:
    client = _init_client()

    # Detect if the view exists by attempting a lightweight select
    use_view = True
    try:
        client.table("view_user_queries").select("id").limit(1).execute()
    except Exception:
        use_view = False

    reports_dir = _ensure_reports_dir()
    ts = datetime.now().strftime("%Y%m%d_%H%M")
    out_path = reports_dir / f"user_queries_{ts}.csv"

    headers = [
        "user_id",
        "username",
        "first_name",
        "agent_type",
        "query",
        "tokens_used",
        "created_at",
    ]

    with out_path.open("w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(headers)
        for page in _fetch_in_pages(client, use_view=use_view):
            for row in page:
                writer.writerow(
                    [
                        row.get("user_id", ""),
                        row.get("username", ""),
                        row.get("first_name", ""),
                        row.get("agent_type", ""),
                        row.get("query", row.get("message", "")),
                        row.get("tokens_used", 0),
                        row.get("created_at", ""),
                    ]
                )

    return out_path


if __name__ == "__main__":
    path = export_user_queries()
    print(f"âœ… Exported user queries to: {path}")



---

## FILE: scripts/export_user_qna.py
"""Export user queries alongside the bot's next answer to a CSV.

This script reads from `view_user_qna` (preferred) and falls back to
joining `conversations` if the view does not exist.
"""

from __future__ import annotations

import csv
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, Iterable, List

from dotenv import load_dotenv
from supabase import Client, create_client

load_dotenv()


def _init_client() -> Client:
    url = os.getenv("SUPABASE_URL")
    key = os.getenv("SUPABASE_SERVICE_KEY")
    if not url or not key:
        raise RuntimeError("Missing SUPABASE_URL or SUPABASE_SERVICE_KEY in environment")
    return create_client(url, key)


def _ensure_reports_dir() -> Path:
    reports = Path(__file__).resolve().parents[1] / "reports"
    reports.mkdir(parents=True, exist_ok=True)
    return reports


def _fetch_qna_pages(client: Client, use_view: bool, page_size: int = 1500) -> Iterable[List[Dict]]:
    last_created_at: str | None = None

    if use_view:
        table = "view_user_qna"
        select = (
            "user_msg_id,user_id,username,first_name,agent_type," \
            "user_query,user_tokens,user_created_at,assistant_msg_id," \
            "assistant_response,assistant_tokens,assistant_created_at"
        )
        while True:
            q = (
                client.table(table)
                .select(select)
            )
            if last_created_at is not None:
                q = q.gt("user_created_at", last_created_at)
            q = q.order("user_created_at", desc=False).limit(page_size)
            res = q.execute()
            rows: List[Dict] = res.data or []
            if not rows:
                break
            last_created_at = rows[-1]["user_created_at"]
            yield rows
    else:
        # Raw fallback: fetch user messages then look ahead for next assistant msg per user/agent
        # Note: less efficient; the view is recommended.
        while True:
            q = (
                client.table("conversations")
                .select("*")
                .eq("role", "user")
            )
            if last_created_at is not None:
                q = q.gt("created_at", last_created_at)
            q = q.order("created_at", desc=False).limit(page_size)
            res = q.execute()
            users: List[Dict] = res.data or []
            if not users:
                break
            last_created_at = users[-1]["created_at"]

            batch: List[Dict] = []
            for u in users:
                # Find next assistant response for same user/agent after this message
                resp = (
                    client.table("conversations")
                    .select("id,message,tokens_used,created_at")
                    .eq("user_id", u["user_id"])\
                    .eq("agent_type", u["agent_type"])\
                    .eq("role", "assistant")\
                    .gt("created_at", u["created_at"])\
                    .order("created_at", desc=False)\
                    .limit(1)\
                    .execute()
                )
                a = resp.data[0] if resp.data else None
                batch.append(
                    {
                        "user_msg_id": u["id"],
                        "user_id": u.get("user_id"),
                        "username": u.get("username"),
                        "first_name": u.get("first_name"),
                        "agent_type": u.get("agent_type"),
                        "user_query": u.get("message", ""),
                        "user_tokens": u.get("tokens_used", 0),
                        "user_created_at": u.get("created_at"),
                        "assistant_msg_id": a.get("id") if a else None,
                        "assistant_response": a.get("message") if a else None,
                        "assistant_tokens": a.get("tokens_used") if a else None,
                        "assistant_created_at": a.get("created_at") if a else None,
                    }
                )
            yield batch


def export_user_qna() -> Path:
    client = _init_client()
    # Detect view
    use_view = True
    try:
        client.table("view_user_qna").select("user_msg_id").limit(1).execute()
    except Exception:
        use_view = False

    reports_dir = _ensure_reports_dir()
    ts = datetime.now().strftime("%Y%m%d_%H%M")
    out_path = reports_dir / f"user_qna_{ts}.csv"

    headers = [
        "user_id",
        "username",
        "first_name",
        "agent_type",
        "user_query",
        "assistant_response",
        "user_tokens",
        "assistant_tokens",
        "user_created_at",
        "assistant_created_at",
    ]

    with out_path.open("w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(headers)
        for page in _fetch_qna_pages(client, use_view=use_view):
            for row in page:
                writer.writerow(
                    [
                        row.get("user_id", ""),
                        row.get("username", ""),
                        row.get("first_name", ""),
                        row.get("agent_type", ""),
                        row.get("user_query", ""),
                        row.get("assistant_response", ""),
                        row.get("user_tokens", 0),
                        row.get("assistant_tokens", 0),
                        row.get("user_created_at", ""),
                        row.get("assistant_created_at", ""),
                    ]
                )

    return out_path


if __name__ == "__main__":
    path = export_user_qna()
    print(f"âœ… Exported user QnA to: {path}")



---

## FILE: scripts/analytics_report.py
"""Generate analytics report from bot usage data."""
import asyncio
import os
from datetime import datetime, timedelta, UTC
from supabase import create_client
from dotenv import load_dotenv

load_dotenv()


async def generate_analytics_report():
    """Generate a comprehensive analytics report."""
    try:
        # Initialize Supabase client
        supabase = create_client(
            os.getenv("SUPABASE_URL"),
            os.getenv("SUPABASE_SERVICE_KEY")
        )
        
        print("ðŸ“Š Bot Analytics Report")
        print("=" * 50)
        
        # Get date ranges
        now = datetime.now(UTC)
        last_24h = now - timedelta(hours=24)
        last_7d = now - timedelta(days=7)
        last_30d = now - timedelta(days=30)
        
        # Total users
        total_users = supabase.table('bot_analytics')\
            .select('user_id', count='exact')\
            .execute()
        
        unique_users = supabase.table('bot_analytics')\
            .select('user_id')\
            .execute()
        
        unique_user_count = len(set(row['user_id'] for row in unique_users.data))
        
        print(f"ðŸ‘¥ Total Users: {unique_user_count}")
        print(f"ðŸ“ˆ Total Events: {total_users.count}")
        
        # Activity by time period
        for period_name, period_date in [("24 hours", last_24h), ("7 days", last_7d), ("30 days", last_30d)]:
            events = supabase.table('bot_analytics')\
                .select('*')\
                .gte('created_at', period_date.isoformat())\
                .execute()
            
            users = set(row['user_id'] for row in events.data)
            print(f"ðŸ“… Last {period_name}: {len(events.data)} events, {len(users)} users")
        
        # Most popular actions
        print(f"\nðŸŽ¯ Popular Actions:")
        actions = supabase.table('bot_analytics')\
            .select('action')\
            .execute()
        
        action_counts = {}
        for row in actions.data:
            action = row['action']
            action_counts[action] = action_counts.get(action, 0) + 1
        
        for action, count in sorted(action_counts.items(), key=lambda x: x[1], reverse=True):
            print(f"   {action}: {count}")
        
        # Agent preferences
        print(f"\nðŸ¤– Agent Preferences:")
        agent_selections = supabase.table('bot_analytics')\
            .select('metadata')\
            .eq('action', 'agent_selected')\
            .execute()
        
        agent_counts = {"pm": 0, "vc": 0}
        for row in agent_selections.data:
            if row['metadata'] and 'agent_type' in row['metadata']:
                agent_type = row['metadata']['agent_type']
                if agent_type in agent_counts:
                    agent_counts[agent_type] += 1
        
        print(f"   ðŸš€ Product Manager: {agent_counts['pm']}")
        print(f"   ðŸ¦ˆ VC/Angel: {agent_counts['vc']}")
        
        # Recent errors
        errors = supabase.table('bot_analytics')\
            .select('*')\
            .eq('action', 'message_error')\
            .gte('created_at', last_7d.isoformat())\
            .execute()
        
        print(f"\nâš ï¸  Errors (last 7 days): {len(errors.data)}")
        
        # Rate limiting
        rate_limits = supabase.table('bot_analytics')\
            .select('*')\
            .eq('action', 'rate_limited')\
            .gte('created_at', last_7d.isoformat())\
            .execute()
        
        print(f"ðŸš« Rate Limits (last 7 days): {len(rate_limits.data)}")
        
        print(f"\nâœ… Report generated successfully!")
        
    except Exception as e:
        print(f"âŒ Error generating analytics report: {e}")


if __name__ == "__main__":
    asyncio.run(generate_analytics_report())

---

## FILE: scripts/test_openrouter.py
"""Test OpenRouter API connection and model availability."""
import asyncio
import os
from openai import AsyncOpenAI
from dotenv import load_dotenv

load_dotenv()


async def test_openrouter_connection():
    """Test OpenRouter API connection and Sonar Pro model."""
    api_key = os.getenv("OPENROUTER_API_KEY")
    
    if not api_key:
        print("âŒ No OPENROUTER_API_KEY found in .env")
        return
    
    try:
        # Initialize OpenRouter client
        client = AsyncOpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=api_key,
        )
        
        print("âœ… OpenRouter client created successfully")
        
        # Test the Sonar Pro model with a simple message
        print("ðŸ§ª Testing perplexity/sonar-pro model...")
        
        response = await client.chat.completions.create(
            model="perplexity/sonar-pro",
            messages=[
                {
                    "role": "system", 
                    "content": "You are a helpful AI assistant."
                },
                {
                    "role": "user", 
                    "content": "Say hello and confirm you're working!"
                }
            ],
            max_tokens=100,
            temperature=0.7,
        )
        
        if response.choices and response.choices[0].message.content:
            print("âœ… Model response received!")
            print(f"ðŸ¤– Response: {response.choices[0].message.content}")
            print(f"ðŸ“Š Tokens used: {response.usage.total_tokens if response.usage else 'Unknown'}")
        else:
            print("âš ï¸  Empty response from model")
            
    except Exception as e:
        print(f"âŒ Error testing OpenRouter: {e}")
        
        # Provide specific guidance based on error type
        error_str = str(e).lower()
        if "401" in error_str or "unauthorized" in error_str:
            print("ðŸ’¡ Tip: Check your OPENROUTER_API_KEY in .env file")
        elif "404" in error_str or "not found" in error_str:
            print("ðŸ’¡ Tip: The model 'perplexity/sonar-pro' might not be available")
            print("   Visit https://openrouter.ai/models to check available models")
        elif "429" in error_str:
            print("ðŸ’¡ Tip: Rate limit reached, wait a moment and try again")


if __name__ == "__main__":
    print("ðŸš€ Testing OpenRouter API connection...")
    print("=" * 50)
    asyncio.run(test_openrouter_connection())

---

## FILE: scripts/test_telegram_bot.py
"""Test Telegram bot connection."""

import asyncio
import os

from dotenv import load_dotenv
from telegram import Bot

load_dotenv()


async def test_bot() -> None:
    """Test bot token and get bot info."""
    token = os.getenv("TELEGRAM_BOT_TOKEN")

    if not token:
        print("âŒ No TELEGRAM_BOT_TOKEN found in .env")
        return

    try:
        bot = Bot(token=token)
        bot_info = await bot.get_me()

        print("âœ… Bot connected successfully!")
        print(f"ðŸ¤– Bot name: {bot_info.first_name}")
        print(f"ðŸ“± Bot username: @{bot_info.username}")
        print(f"ðŸ†” Bot ID: {bot_info.id}")
        print(f"ðŸ’¬ Can join groups: {bot_info.can_join_groups}")
        print(f"ðŸ“– Can read all group messages: {bot_info.can_read_all_group_messages}")

    except Exception as e:
        print(f"âŒ Error connecting to bot: {e}")


if __name__ == "__main__":
    asyncio.run(test_bot())

---

## FILE: scripts/run_migrations.py
"""Display SQL migrations that need to be run manually in Supabase."""

import os
from pathlib import Path
from supabase import create_client
from dotenv import load_dotenv

load_dotenv()


def check_tables_exist():
    """Check which tables exist in the database."""
    try:
        supabase = create_client(
            os.getenv("SUPABASE_URL"),
            os.getenv("SUPABASE_SERVICE_KEY")
        )
        
        print("ðŸ” Checking existing tables...")
        
        # Try to query each expected table
        tables_to_check = ['conversations', 'user_sessions', 'bot_analytics']
        existing_tables = []
        
        for table in tables_to_check:
            try:
                result = supabase.table(table).select('*').limit(1).execute()
                existing_tables.append(table)
                print(f"   âœ… {table} - exists")
            except Exception as e:
                print(f"   âŒ {table} - missing: {str(e)}")
        
        return existing_tables
        
    except Exception as e:
        print(f"âŒ Error checking tables: {e}")
        return []


def display_migrations():
    """Display SQL migration content for manual execution."""
    try:
        print("\nðŸš€ Database Migration Setup")
        print("=" * 50)
        
        # Check existing tables
        existing_tables = check_tables_exist()
        
        # Get migrations directory
        migrations_dir = Path(__file__).parent.parent / "migrations"
        
        if not migrations_dir.exists():
            print("âŒ Migrations directory not found!")
            return
        
        # Get all SQL files and sort them
        migration_files = sorted(migrations_dir.glob("*.sql"))
        
        if not migration_files:
            print("âŒ No migration files found!")
            return
        
        print(f"\nðŸ“„ Found {len(migration_files)} migration files:")
        
        # Display each migration
        for migration_file in migration_files:
            print(f"\n{'='*60}")
            print(f"ðŸ“„ {migration_file.name}")
            print(f"{'='*60}")
            
            # Check if this migration is needed
            if migration_file.name == "001_initial_schema.sql":
                needed = not all(table in existing_tables for table in ['conversations', 'user_sessions'])
            elif migration_file.name == "002_analytics_table.sql":
                needed = 'bot_analytics' not in existing_tables
            else:
                needed = True
            
            if needed:
                print("ðŸŸ¡ STATUS: NEEDS TO BE RUN")
            else:
                print("ðŸŸ¢ STATUS: ALREADY APPLIED")
            
            print("\nðŸ“ SQL Content:")
            print("-" * 40)
            
            # Read and display the SQL content
            sql_content = migration_file.read_text()
            print(sql_content)
            
            print("-" * 40)
        
        print(f"\nðŸ“‹ INSTRUCTIONS:")
        print("1. Open your Supabase dashboard")
        print("2. Go to SQL Editor")
        print("3. Copy and paste each migration marked as 'NEEDS TO BE RUN'")
        print("4. Execute the SQL in order (001, 002, etc.)")
        print("5. Run the analytics report again")
        
        if 'bot_analytics' not in existing_tables:
            print(f"\nâš ï¸  IMPORTANT: The bot_analytics table is missing!")
            print("   This is why your analytics report is failing.")
            print("   Please run migration 002_analytics_table.sql")
        
    except Exception as e:
        print(f"âŒ Error displaying migrations: {e}")


if __name__ == "__main__":
    display_migrations()

---

## FILE: scripts/test_enhanced_bot.py
"""Test enhanced bot features."""
import asyncio
import logging
import sys
import os

# Add the project root to the Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

# Enable debug logging to see citation processing
logging.basicConfig(level=logging.DEBUG, format='%(name)s - %(levelname)s - %(message)s')

from src.bot.agents import AIAgent

async def test_enhanced_formatting():
    """Test the new formatting capabilities."""
    ai = AIAgent()
    
    # Test response formatting
    test_content = """
What problem are you solving?
Who is your target user?
Let's talk about Jobs-to-be-Done framework.
Here are some key points:
- First point about growth
- Second point about retention
What's your CAC to LTV ratio?
"""
    
    formatted = ai.format_response(test_content, "pm")
    print("Formatted PM Response:")
    print(formatted)
    print("\n" + "="*50 + "\n")
    
    formatted_vc = ai.format_response(test_content, "vc")
    print("Formatted VC Response:")
    print(formatted_vc)
    
    # Test citation parsing
    print("\n" + "="*50 + "\n")
    test_citations = """This is a fact about startups [1] and another insight [2].

Sources:
1. TechCrunch Article - https://techcrunch.com/example
2. Harvard Business Review - https://hbr.org/example"""
    
    print("Citation Test Input:")
    print(repr(test_citations))
    print("\nCitation Test Output:")
    citation_formatted = ai.format_response(test_citations, "pm")
    print(citation_formatted)
    
    # Test direct citation extraction
    print("\n" + "="*30 + "\n")
    print("Direct Citation Extraction Test:")
    extracted = ai.extract_and_format_citations(test_citations)
    print(extracted)
    
    # Test reference cleaning
    print("\n" + "="*30 + "\n")
    test_references = """â€¢ **For Users:** There is no guarantee of ownership, ability to contribute to or sustain digital organisms, or assurance that their efforts/creations will last[1][3].
â€¢ This is another point with references[2][4][5].
â€¢ Single reference here[1]."""
    
    print("Reference Cleaning Test Input:")
    print(test_references)
    print("\nReference Cleaning Test Output:")
    cleaned = ai.clean_references(test_references)
    print(cleaned)

if __name__ == "__main__":
    asyncio.run(test_enhanced_formatting())

---

## FILE: migrations/004_user_queries_view.sql
-- Read-friendly view for exporting user queries
-- Creates a view that selects only user-side messages with useful derived fields

CREATE OR REPLACE VIEW view_user_queries AS
SELECT
    c.id,
    c.user_id,
    c.username,
    c.first_name,
    c.agent_type,
    c.message AS query,
    c.tokens_used,
    c.created_at,
    length(c.message) AS message_length,
    date_trunc('day', c.created_at) AS day,
    date_trunc('hour', c.created_at) AS hour
FROM conversations c
WHERE c.role = 'user';

-- Optional: if you need a materialized view for very large datasets, create it instead
-- and remember to REFRESH MATERIALIZED VIEW periodically.


---

## FILE: migrations/005_user_qna_view.sql
-- View that pairs each user message with the immediate next assistant response

CREATE OR REPLACE VIEW view_user_qna AS
WITH ordered AS (
    SELECT
        c.*,
        lead(c.role) OVER (PARTITION BY c.user_id, c.agent_type ORDER BY c.created_at) AS next_role,
        lead(c.id) OVER (PARTITION BY c.user_id, c.agent_type ORDER BY c.created_at) AS assistant_msg_id,
        lead(c.message) OVER (PARTITION BY c.user_id, c.agent_type ORDER BY c.created_at) AS assistant_response,
        lead(c.tokens_used) OVER (PARTITION BY c.user_id, c.agent_type ORDER BY c.created_at) AS assistant_tokens,
        lead(c.created_at) OVER (PARTITION BY c.user_id, c.agent_type ORDER BY c.created_at) AS assistant_created_at
    FROM conversations c
)
SELECT
    id AS user_msg_id,
    user_id,
    username,
    first_name,
    agent_type,
    message AS user_query,
    tokens_used AS user_tokens,
    created_at AS user_created_at,
    assistant_msg_id,
    assistant_response,
    assistant_tokens,
    assistant_created_at
FROM ordered
WHERE role = 'user' AND next_role = 'assistant';


---

## FILE: migrations/002_analytics_table.sql
-- Analytics table for tracking bot usage
-- This file contains the schema for bot analytics and usage tracking

-- Bot analytics table to store user interaction events
CREATE TABLE IF NOT EXISTS bot_analytics (
    id UUID DEFAULT gen_random_uuid() PRIMARY KEY,
    user_id TEXT NOT NULL,
    action TEXT NOT NULL,
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Indexes for better performance on analytics queries
CREATE INDEX IF NOT EXISTS idx_bot_analytics_user_id ON bot_analytics(user_id);
CREATE INDEX IF NOT EXISTS idx_bot_analytics_action ON bot_analytics(action);
CREATE INDEX IF NOT EXISTS idx_bot_analytics_created_at ON bot_analytics(created_at);
CREATE INDEX IF NOT EXISTS idx_bot_analytics_user_action ON bot_analytics(user_id, action);

-- Analytics events that will be tracked:
-- - bot_started: When user first starts the bot
-- - agent_selected: When user selects PM or VC agent
-- - agent_switched: When user switches between agents  
-- - message_processed: When user sends message and gets AI response
-- - message_error: When message processing fails
-- - conversation_reset: When user resets conversation history
-- - stats_viewed: When user checks their statistics
-- - rate_limited: When user hits rate limit (optional enhancement)

---

## FILE: migrations/001_initial_schema.sql
-- Initial schema for Telegram AI Bot
-- This file contains the database schema for conversations and user sessions

-- Conversations table to store all chat messages
CREATE TABLE IF NOT EXISTS conversations (
    id UUID DEFAULT gen_random_uuid() PRIMARY KEY,
    user_id TEXT NOT NULL,
    username TEXT,
    first_name TEXT,
    agent_type TEXT NOT NULL,
    role TEXT NOT NULL CHECK (role IN ('user', 'assistant')),
    message TEXT NOT NULL,
    tokens_used INTEGER DEFAULT 0,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- User sessions table to track active users and their current agent
CREATE TABLE IF NOT EXISTS user_sessions (
    id UUID DEFAULT gen_random_uuid() PRIMARY KEY,
    user_id TEXT UNIQUE NOT NULL,
    username TEXT,
    first_name TEXT,
    current_agent TEXT NOT NULL,
    last_active TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Indexes for better performance
CREATE INDEX IF NOT EXISTS idx_conversations_user_id ON conversations(user_id);
CREATE INDEX IF NOT EXISTS idx_conversations_agent_type ON conversations(agent_type);
CREATE INDEX IF NOT EXISTS idx_conversations_created_at ON conversations(created_at);
CREATE INDEX IF NOT EXISTS idx_user_sessions_user_id ON user_sessions(user_id);
CREATE INDEX IF NOT EXISTS idx_user_sessions_last_active ON user_sessions(last_active);
---

## FILE: migrations/003_conversation_summaries.sql
-- Add conversation summaries and metadata
ALTER TABLE conversations 
ADD COLUMN IF NOT EXISTS metadata JSONB DEFAULT '{}';

ALTER TABLE user_sessions
ADD COLUMN IF NOT EXISTS conversation_context JSONB DEFAULT '{}';

-- Index for faster JSON queries
CREATE INDEX IF NOT EXISTS idx_conversations_metadata ON conversations USING GIN (metadata);

---

## FILE: ./main.py
def main():
    print("Hello from telegram-ai-bot-v2!")


if __name__ == "__main__":
    main()

---

